{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPFFxiixwtE1IzteYc8o2FM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devaru-ai/cuda-inference/blob/main/SIMT-Kernels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1 Prove that variables are expensive**\n",
        "\n",
        "## Skinny Kernel\n",
        "\n",
        "1. Thread with small no of registers\n",
        "2. High Occupancy\n",
        "\n",
        "## Fat Kernel\n",
        "\n",
        "1. Thread declares 20 local variables\n",
        "2. Crashes at ~5-10%"
      ],
      "metadata": {
        "id": "7cUY6L3dpWcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 1 : We got a silent crash!**\n",
        "\n",
        "We use volatile keyword to force the compiler to be dumb.\n",
        "\n",
        "Even if its not used, we tell it to store the variable in a register.\n",
        "\n",
        "We create a dependency chain using the for loop to keep all those variables alive in the register. Eg: v1 += v2 ie, to update v1, it needs v2.\n",
        "\n",
        "# **Debug:**\n",
        "\n",
        "## **The 0.0% Occupancy.**\n",
        "\n",
        "Func Call: cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, ...);\n",
        "\n",
        "- I'm running on A100.\n",
        "\n",
        "- By default nvcc often compiles for old architecture.\n",
        "\n",
        "- So, A100 driver looked at code and was like \"Bruhh.. this code is compiled for an old GPU, I can't calculate for A100 using this old blueprint.\"\n",
        "\n",
        "- The func failed.\n",
        "\n",
        "- Bcz, the func failed, it never wrote a val into numBlocks.\n",
        "\n",
        "- numBlocks remained at its default 0.\n",
        "\n",
        "- The math: 0 * 256 * 100/MaxThreads = 0%.\n",
        "\n",
        "**So, next we are gonna add the flag --arch=sm_80.**"
      ],
      "metadata": {
        "id": "Pql2dEKz2vBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile register_test.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --- KERNEL 1: The \"Lean\" Kernel (Small Luggage) ---\n",
        "// Uses minimal variables. Should fit many threads.\n",
        "__global__ void lean_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "\n",
        "    // Just one variable. The compiler loves this.\n",
        "    float val = tid * 1.0f;\n",
        "    val += 10.0f;\n",
        "    out[tid] = val;\n",
        "}\n",
        "\n",
        "// --- KERNEL 2: The \"Fat\" Kernel (Big Luggage) ---\n",
        "// Uses tons of variables. The SM will run out of space.\n",
        "__global__ void fat_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "\n",
        "    // \"volatile\" tells the compiler: \"Do NOT optimize these away. Keep them in registers.\"\n",
        "    // We are forcing each thread to hold onto 16 different numbers at once.\n",
        "    volatile float v0 = tid * 1.0f;\n",
        "    volatile float v1 = tid * 2.0f;\n",
        "    volatile float v2 = tid * 3.0f;\n",
        "    volatile float v3 = tid * 4.0f;\n",
        "    volatile float v4 = tid * 5.0f;\n",
        "    volatile float v5 = tid * 6.0f;\n",
        "    volatile float v6 = tid * 7.0f;\n",
        "    volatile float v7 = tid * 8.0f;\n",
        "    volatile float v8 = tid * 9.0f;\n",
        "    volatile float v9 = tid * 10.0f;\n",
        "    volatile float v10 = tid * 11.0f;\n",
        "    volatile float v11 = tid * 12.0f;\n",
        "    volatile float v12 = tid * 13.0f;\n",
        "    volatile float v13 = tid * 14.0f;\n",
        "    volatile float v14 = tid * 15.0f;\n",
        "    volatile float v15 = tid * 16.0f;\n",
        "\n",
        "    // Fake math loop to keep variables alive\n",
        "    for(int i=0; i<100; i++) {\n",
        "        v0 += v1; v1 += v2; v2 += v3; v3 += v4;\n",
        "        v4 += v5; v5 += v6; v6 += v7; v7 += v8;\n",
        "        v8 += v9; v9 += v10; v10 += v11; v11 += v12;\n",
        "        v12 += v13; v13 += v14; v14 += v15; v15 += v0;\n",
        "    }\n",
        "\n",
        "    out[tid] = v0 + v1 + v2 + v3;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int dev_id = 0;\n",
        "    cudaSetDevice(dev_id);\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, dev_id);\n",
        "\n",
        "    printf(\"GPU Model: %s\\n\", prop.name);\n",
        "    printf(\"Total Register Capacity per SM: %d\\n\", prop.regsPerMultiprocessor);\n",
        "    printf(\"------------------------------------------------\\n\");\n",
        "\n",
        "    int numBlocks;\n",
        "    int blockSize = 256; // Standard block size\n",
        "\n",
        "    // --- MEASURE LEAN KERNEL ---\n",
        "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, lean_kernel, blockSize, 0);\n",
        "\n",
        "    float occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[LEAN KERNEL] Max Blocks per SM: %d  |  Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    // --- MEASURE FAT KERNEL ---\n",
        "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, fat_kernel, blockSize, 0);\n",
        "\n",
        "    occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[FAT KERNEL]  Max Blocks per SM: %d  |  Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    printf(\"------------------------------------------------\\n\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YILY4PpQqBJO",
        "outputId": "1e5ed820-fb4e-479c-f586-216feef54694"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting register_test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc register_test.cu -o register_test\n",
        "!./register_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Dkt-InGgx17J",
        "outputId": "8b33c1c7-c90a-4ed1-9b8f-547413cec887"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Model: NVIDIA A100-SXM4-40GB\n",
            "Total Register Capacity per SM: 65536\n",
            "------------------------------------------------\n",
            "[LEAN KERNEL] Max Blocks per SM: 0  |  Occupancy: 0.0%\n",
            "[FAT KERNEL]  Max Blocks per SM: 0  |  Occupancy: 0.0%\n",
            "------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 2: A100 is quite the beast!**\n",
        "\n",
        "Basically, it saw the Fat Kernel, and was like, Is that all you got?\n",
        "\n",
        "# **The Math**\n",
        "\n",
        "## **The Hardware Limit (A100 Stats)**\n",
        "\n",
        "1. Total Reg per SM = 65,536\n",
        "2. Max Threads allowed per SM = 2,048\n",
        "3. To hit 100% occupancy: each thread must use fewer than 65,536/2,048 = 32 registers.\n",
        "\n",
        "I declared 16 variables, that's 16-20 registers (16 for var + a few for loop indices)\n",
        "\n",
        "Is 20 < 32? Hmm, That's skinny for A100.\n",
        "\n",
        "So, 20 * 2048 = 40,960 registers.\n",
        "\n",
        "We have 65,536 registers available.\n",
        "\n",
        "Next, we are gonna force the code to use 72 registers.\n",
        "\n",
        "# **The Big Learning: Register Pressure is Relative.**\n",
        "\n",
        "On older GPU like T4 or V100, the 16-var code might have dropped occupancy to 75% or 50%.\n",
        "\n",
        "A100 is a very forgiving chip."
      ],
      "metadata": {
        "id": "4zUU3jGE59T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile register_test.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --- KERNEL 1: Lean ---\n",
        "__global__ void lean_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "    float val = tid * 1.0f;\n",
        "    val += 10.0f;\n",
        "    out[tid] = val;\n",
        "}\n",
        "\n",
        "// --- KERNEL 2: Fat ---\n",
        "__global__ void fat_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "\n",
        "    // Force high register usage\n",
        "    volatile float v0 = tid * 1.0f; volatile float v1 = tid * 2.0f;\n",
        "    volatile float v2 = tid * 3.0f; volatile float v3 = tid * 4.0f;\n",
        "    volatile float v4 = tid * 5.0f; volatile float v5 = tid * 6.0f;\n",
        "    volatile float v6 = tid * 7.0f; volatile float v7 = tid * 8.0f;\n",
        "    volatile float v8 = tid * 9.0f; volatile float v9 = tid * 10.0f;\n",
        "    volatile float v10 = tid * 11.0f; volatile float v11 = tid * 12.0f;\n",
        "    volatile float v12 = tid * 13.0f; volatile float v13 = tid * 14.0f;\n",
        "    volatile float v14 = tid * 15.0f; volatile float v15 = tid * 16.0f;\n",
        "\n",
        "    // Fake math loop\n",
        "    for(int i=0; i<100; i++) {\n",
        "        v0 += v1; v1 += v2; v2 += v3; v3 += v4;\n",
        "        v4 += v5; v5 += v6; v6 += v7; v7 += v8;\n",
        "        v8 += v9; v9 += v10; v10 += v11; v11 += v12;\n",
        "        v12 += v13; v13 += v14; v14 += v15; v15 += v0;\n",
        "    }\n",
        "    out[tid] = v0 + v1 + v2 + v3;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int dev_id = 0;\n",
        "    cudaSetDevice(dev_id);\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, dev_id);\n",
        "\n",
        "    printf(\"GPU Model: %s\\n\", prop.name);\n",
        "\n",
        "    int numBlocks;\n",
        "    int blockSize = 256;\n",
        "    cudaError_t err;\n",
        "\n",
        "    // --- LEAN KERNEL ---\n",
        "    err = cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, lean_kernel, blockSize, 0);\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"FAILED (Lean): %s\\n\", cudaGetErrorString(err));\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    float occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[LEAN KERNEL] Blocks: %d | Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    // --- FAT KERNEL ---\n",
        "    err = cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, fat_kernel, blockSize, 0);\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"FAILED (Fat): %s\\n\", cudaGetErrorString(err));\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[FAT KERNEL]  Blocks: %d | Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7RZtz6bpyR9e",
        "outputId": "b9054366-2f35-4ddf-a750-928554f0b901"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting register_test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 register_test.cu -o register_test\n",
        "!./register_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BbGq_1Btu6EF",
        "outputId": "d93f2f05-0bea-4dcb-b2f5-28031c5de4e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Model: NVIDIA A100-SXM4-40GB\n",
            "[LEAN KERNEL] Blocks: 8 | Occupancy: 100.0%\n",
            "[FAT KERNEL]  Blocks: 8 | Occupancy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 3: I tried to trick the Compile into using 100 registers**\n",
        "\n",
        "## It analysed the logic and realised it could do the exact same math using only 2 registers.\n",
        "\n",
        "# **The Bluff**\n",
        "\n",
        "float data[100];\n",
        "\n",
        "- I thought it would reserve space for 100 sep numbers.\n",
        "\n",
        "\n",
        "Fatal Flaw:\n",
        "data[i+1] += data[i] * 0.5f;\n",
        "\n",
        "- Calculate data[0]. Put it in R1\n",
        "- Next, data[1] = data[1] + (data[0] * 0.5)\n",
        "- Read R1, do the math, put it in R2.\n",
        "- Looks ahead, Does anybody need data[0] (R1) ever again? No? Nobody.. Marks R1 as FREE.\n",
        "\n",
        "So, instead of 100 registers like I thought it would, it just used around ~10 (for math + indices). So, Occupancy: 100%\n"
      ],
      "metadata": {
        "id": "jakdihBD8dP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile register_test.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --- KERNEL 1: Lean (Control Group) ---\n",
        "__global__ void lean_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "    float val = tid * 1.0f;\n",
        "    val += 10.0f;\n",
        "    out[tid] = val;\n",
        "}\n",
        "\n",
        "// --- KERNEL 2: Super Fat (Experimental Group) ---\n",
        "// We use a local array of 100 floats.\n",
        "// If the compiler puts this in registers, it uses ~100 registers per thread.\n",
        "// 100 regs * 2048 threads = 200,000 regs needed.\n",
        "// The SM only has 65,536.\n",
        "// RESULT: The SM MUST reduce the number of active threads (Occupancy crash).\n",
        "__global__ void fat_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "\n",
        "    // Declare a massive amount of local state\n",
        "    float data[100];\n",
        "\n",
        "    // Initialize (prevent optimization)\n",
        "    #pragma unroll\n",
        "    for(int i=0; i<100; i++) {\n",
        "        data[i] = tid * 0.001f + i;\n",
        "    }\n",
        "\n",
        "    // Heavy computation to keep registers \"alive\"\n",
        "    #pragma unroll\n",
        "    for(int i=0; i<99; i++) {\n",
        "        data[i+1] += data[i] * 0.5f;\n",
        "    }\n",
        "\n",
        "    out[tid] = data[99];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int dev_id = 0;\n",
        "    cudaSetDevice(dev_id);\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, dev_id);\n",
        "\n",
        "    printf(\"GPU: %s\\n\", prop.name);\n",
        "\n",
        "    int numBlocks;\n",
        "    int blockSize = 256;\n",
        "\n",
        "    // --- LEAN ---\n",
        "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, lean_kernel, blockSize, 0);\n",
        "    float occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[LEAN KERNEL] Blocks: %d | Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    // --- FAT ---\n",
        "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, fat_kernel, blockSize, 0);\n",
        "    occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[FAT KERNEL]  Blocks: %d | Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dE2iEppcyhUC",
        "outputId": "e064e4a6-8b64-4db2-9b36-fac67bb59117"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting register_test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 --ptxas-options=-v register_test.cu -o register_test\n",
        "!./register_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vaoyqXvFyijY",
        "outputId": "c09cb288-572e-4346-b4d0-0173ac652316"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z10fat_kernelPfi' for 'sm_80'\n",
            "ptxas info    : Function properties for _Z10fat_kernelPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 364 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z11lean_kernelPfi' for 'sm_80'\n",
            "ptxas info    : Function properties for _Z11lean_kernelPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 8 registers, 364 bytes cmem[0]\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "[LEAN KERNEL] Blocks: 8 | Occupancy: 100.0%\n",
            "[FAT KERNEL]  Blocks: 8 | Occupancy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 4: Victoryyy!**\n",
        "\n",
        "1. Declare 128 vars\n",
        "2. #pragma unroll -> Instead of using a loop counter, we make the compiler copy paste the lines 128 times, creating 128 independent assignments.\n",
        "3. Forces compiler to treat val[0] through val[127] as sep vars(registers)\n",
        "\n",
        "- The Logic:\n",
        "\n",
        "1. When i = 0: We update vals[0] using vals[64].\n",
        "\n",
        "2. When i = 64: We update vals[64] using vals[0].\n",
        "\n",
        "\n",
        "# **The Math**\n",
        "\n",
        "72 reg per thread\n",
        "\n",
        "156 threads * 72 regs = 18,432 regs per block\n",
        "\n",
        "Total SM capacity = 65, 536 registers\n",
        "\n",
        "65,536 / 18, 432 = 3.55\n",
        "\n",
        "GPU rounds down to 3 blocks\n",
        "\n",
        "3 blocks * 256 threads = 768 threads\n",
        "\n",
        "768 / 2048 (Max) = 37.5%\n",
        "\n",
        "# **Conclusion**\n",
        "\n",
        "By forcing each thread to carry \"72 registers\", the SM could only fit a few threads, leaving 60%+ empty."
      ],
      "metadata": {
        "id": "0zpbpYxv_13y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile register_test.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --- KERNEL 1: Lean ---\n",
        "__global__ void lean_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "    float val = tid * 1.0f;\n",
        "    val += 10.0f;\n",
        "    out[tid] = val;\n",
        "}\n",
        "\n",
        "// --- KERNEL 2: Fat ---\n",
        "// Goal: Burn > 64 registers per thread to crash occupancy below 50%\n",
        "__global__ void fat_kernel(float *out, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "\n",
        "    // 1. Declare 128 variables (Massive state)\n",
        "    float vals[128];\n",
        "\n",
        "    // 2. Initialize them (Prevent dead code elimination)\n",
        "    #pragma unroll\n",
        "    for (int i = 0; i < 128; i++) {\n",
        "        vals[i] = tid * 0.0001f + i;\n",
        "    }\n",
        "\n",
        "    // 3. The \"Liveness\" Trap\n",
        "    // We update vals[i] using vals[i + 64].\n",
        "    // This forces the compiler to keep 'vals[0]' alive until we reach 'vals[64]'.\n",
        "    // It CANNOT reuse the register for vals[0] yet.\n",
        "    #pragma unroll\n",
        "    for (int k = 0; k < 5; k++) { // Repeat to add complexity\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < 128; i++) {\n",
        "            int neighbor = (i + 64) % 128; // Look 64 steps away\n",
        "            vals[i] += vals[neighbor] * 0.001f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // 4. Sum them up (Output dependency)\n",
        "    float sum = 0.0f;\n",
        "    #pragma unroll\n",
        "    for (int i = 0; i < 128; i++) {\n",
        "        sum += vals[i];\n",
        "    }\n",
        "\n",
        "    out[tid] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int dev_id = 0;\n",
        "    cudaSetDevice(dev_id);\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, dev_id);\n",
        "\n",
        "    printf(\"GPU: %s\\n\", prop.name);\n",
        "\n",
        "    int numBlocks;\n",
        "    int blockSize = 256;\n",
        "\n",
        "    // --- LEAN ---\n",
        "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, lean_kernel, blockSize, 0);\n",
        "    float occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[LEAN KERNEL] Blocks: %d | Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    // --- FAT ---\n",
        "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, fat_kernel, blockSize, 0);\n",
        "    occupancy = (numBlocks * blockSize * 100.0f) / prop.maxThreadsPerMultiProcessor;\n",
        "    printf(\"[FAT KERNEL]  Blocks: %d | Occupancy: %.1f%%\\n\", numBlocks, occupancy);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xHkhuLj1y2og",
        "outputId": "03bb4479-7e43-4718-ec1c-f2a0abe9e562"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting register_test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 --ptxas-options=-v register_test.cu -o register_test\n",
        "!./register_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HSzG9AYcy5Qz",
        "outputId": "3dabfba2-e803-4e42-b245-24822e5783c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z10fat_kernelPfi' for 'sm_80'\n",
            "ptxas info    : Function properties for _Z10fat_kernelPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 72 registers, 364 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z11lean_kernelPfi' for 'sm_80'\n",
            "ptxas info    : Function properties for _Z11lean_kernelPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 8 registers, 364 bytes cmem[0]\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "[LEAN KERNEL] Blocks: 8 | Occupancy: 100.0%\n",
            "[FAT KERNEL]  Blocks: 3 | Occupancy: 37.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.2 Make Half Go Left and Half Go Right**"
      ],
      "metadata": {
        "id": "-5h3IxAdDQcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Warp Divergence**\n",
        "\n",
        "# 1. Coherent Warp\n",
        "\n",
        "We have 32 threads in a warp.\n",
        "- if (tid < 32)\n",
        "- It's gonna be True for them all.\n",
        "\n",
        "The Warp executes the True block once.\n",
        "\n",
        "**Time Taken: 1 Unit.**\n",
        "\n",
        "# 2. Divergent Warp\n",
        "- if (tid % 2 == 0)\n",
        "- For even threads, its True\n",
        "- For odd ones, it's False\n",
        "\n",
        "The Warp has to serialize in this case.\n",
        "\n",
        "**Step A = True Path:**\n",
        "- It masks all odd ones\n",
        "- Even threads works while odd ones sit idle.\n",
        "- Time Taken: 1 Unit.\n",
        "\n",
        "**Step B = False Path:**\n",
        "- Hardware flips the mask, Now evens are masked.\n",
        "- Even threads are idle, Odd ones work now.\n",
        "- Time Taken: 1 Unit.\n",
        "\n",
        "**Total Time: Step A (1 Unit) + Step B (1 Unit) = 2 Units.**\n",
        "\n",
        "\n",
        "**You paid for 100% of the chip, but you only used 50% of it at any given moment.**\n",
        "\n",
        "## Wasting Moneyyyy!\n"
      ],
      "metadata": {
        "id": "eFWZp-gGDQlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I'm gonna tell something cool now, So Listen Close!\n",
        "\n",
        "# It's PTX\n",
        "\n",
        "Parallel Thread Execution.\n",
        "\n",
        "- It's like the virtual assembly language for NVIDIA GPUs.\n",
        "\n",
        "- So you have the C++/CUDA code - That's high level human logic\n",
        "\n",
        "- Then you have PTX, essentially what the compiler (nvcc) produces. It's low level assembly lang, but it's generic. It doesn't know if you are running on GTX 1080 or an A100. It just describes the instructions.\n",
        "\n",
        "- SASS (Haha, who named it?): It's Streaming Assembly. The absolute bottom layer. The GPU Driver produces binary that the silicon executes."
      ],
      "metadata": {
        "id": "xxmCJfWQvqt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTX is a lie and SASS is the harsh reality.\n",
        "\n",
        "- PTX pretends like you have infinite registers like %r9999 - valid!\n",
        "\n",
        "- SASS knows the truth. It's like I got your map to infinite variables to fit into **256** physical slots R0 through R255.\n",
        "\n",
        "But SASS is smart. And nice too! And it makes things possible, not complain!\n",
        "\n",
        "# **Register Reallocation**\n",
        "## One of the difficult but cool math problems\n",
        "## It's not rocket science, It either does **Register reuse** after one operation or it does **Register Spilling**\n",
        "\n",
        "# Register spilling is disastrous for performance.\n",
        "\n",
        "- Say the code has 300 vars in one line of SASS code.\n",
        "- It kicks out 45 back to global memory.\n",
        "- Y'all know HBM/DRAM is slowwww.\n",
        "\n",
        "- Register Access: ~1 Clock Cycle (On-chip, next to the ALU).\n",
        "\n",
        "- Local Memory Access: ~200â€“600 Clock Cycles (Off-chip, traveling to VRAM and back).\n",
        "\n",
        "- The Stall: The CUDA Core (ALU) sits idle for those 600 cycles, doing absolutely nothing, waiting for the data to arrive. This is why performance collapses.\n",
        "\n"
      ],
      "metadata": {
        "id": "tg6wzK5dyWLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, I got distracted by PTX, I just wanted to talk abt __nanosleep(ns):\n",
        "\n",
        "- It just tells Warp Scheduler to not issue any more instructions for this warp for X clock cycles\n",
        "\n",
        "- So the warp stays active but it just goes to stall stanvte.\n",
        "\n",
        "- nvcc is an aggressive optimizer, so when it sees my math loop it could be like you calculated a but you never used it later. This loop does nothing useful. **DELETE**. It erases the loop and it takes 0 ns.\n",
        "\n",
        "- But if I insert nanosleep, it can't delete it cuz that would change the program behavior."
      ],
      "metadata": {
        "id": "jtqi95zQ1xwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 1 : I got \"Optimized, Compiler Wins\"**\n",
        "\n",
        "- The first kernel we launch in any program pays the Driver Tax. GPU had to wake up, load context and initialise the clock. So that took extra 0.38 ms. So, thats why coherent kernel took 1.38 ms.\n",
        "\n",
        "- The Divergent kernel took 1.019 ms. That's like 1 trip. Not 2.\n",
        "\n",
        "- The compiler looked at my code and was like Wait, if you are even you sleep, and if you are odd you sleep. I'm deleting if/else logic and just running sleep (1ms) for everyone.\n",
        "\n",
        "- So, I wanted to split but it taped it back together.\n",
        "\n",
        "\n",
        "## **Next try I'm gonna add a Warmup kernel and make the branches diff So compiler cannot merge them.**\n",
        "\n"
      ],
      "metadata": {
        "id": "ojX0vkeu81xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile warp_divergence.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --- HELPER: The Time Anchor ---\n",
        "// This acts as a \"heavy math operation\" that takes exactly 1ms.\n",
        "// The compiler cannot delete this because it has a side effect (time).\n",
        "__device__ void burn_time() {\n",
        "    __nanosleep(1000000); // 1,000,000 nanoseconds = 1 millisecond\n",
        "}\n",
        "\n",
        "// --- KERNEL A: Coherent (The Unanimous Vote) ---\n",
        "// Condition: tid < 32\n",
        "// Result: In a warp of 32 threads, ALL threads evaluate this to TRUE.\n",
        "// Execution: The warp runs the TRUE block once.\n",
        "// Cost: 1 Trip.\n",
        "__global__ void coherent_kernel() {\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (tid < 32) {\n",
        "        burn_time();\n",
        "    } else {\n",
        "        burn_time(); // Dead code (never reached by this warp)\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- KERNEL B: Divergent (The Split Vote) ---\n",
        "// Condition: tid % 2 == 0\n",
        "// Result: 16 threads are Even (True), 16 threads are Odd (False).\n",
        "// Execution:\n",
        "//    1. Hardware masks off Odds. Runs Evens (1ms).\n",
        "//    2. Hardware masks off Evens. Runs Odds (1ms).\n",
        "// Cost: 2 Trips.\n",
        "__global__ void divergent_kernel() {\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (tid % 2 == 0) {\n",
        "        burn_time();\n",
        "    } else {\n",
        "        burn_time();\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // 1. Setup Timers (CUDA Events are precise hardware timestamps)\n",
        "    float time_coherent, time_divergent;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    printf(\"--- WARP DIVERGENCE BENCHMARK ---\\n\");\n",
        "    printf(\"Task: Force the GPU to sleep for 1ms inside an if/else.\\n\");\n",
        "    printf(\"Hypothesis: Divergent kernel should take 2x longer.\\n\");\n",
        "    printf(\"-------------------------------------\\n\");\n",
        "\n",
        "    // 2. Run Coherent Kernel (1 Block, 32 Threads = 1 Warp)\n",
        "    cudaEventRecord(start);\n",
        "    coherent_kernel<<<1, 32>>>();\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_coherent, start, stop);\n",
        "\n",
        "    printf(\"Coherent Time:  %.3f ms\\n\", time_coherent);\n",
        "\n",
        "    // 3. Run Divergent Kernel (1 Block, 32 Threads = 1 Warp)\n",
        "    cudaEventRecord(start);\n",
        "    divergent_kernel<<<1, 32>>>();\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_divergent, start, stop);\n",
        "\n",
        "    printf(\"Divergent Time: %.3f ms\\n\", time_divergent);\n",
        "\n",
        "    // 4. Validate\n",
        "    float ratio = time_divergent / time_coherent;\n",
        "    printf(\"-------------------------------------\\n\");\n",
        "    printf(\"Slowdown Factor: %.2fx\\n\", ratio);\n",
        "\n",
        "    if (ratio > 1.9) {\n",
        "        printf(\"CONCLUSION: SUCCESS. The hardware serialized the execution paths.\\n\");\n",
        "    } else {\n",
        "        printf(\"CONCLUSION: FAILED. Check compiler settings.\\n\");\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyU3gcPAPcig",
        "outputId": "366b3f32-167a-43dd-e837-d58e32963d17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting warp_divergence.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 warp_divergence.cu -o warp_divergence\n",
        "!./warp_divergence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXgmnpG68aqL",
        "outputId": "71bfee19-cc2c-4b04-9faa-d508171808ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- WARP DIVERGENCE BENCHMARK ---\n",
            "Task: Force the GPU to sleep for 1ms inside an if/else.\n",
            "Hypothesis: Divergent kernel should take 2x longer.\n",
            "-------------------------------------\n",
            "Coherent Time:  1.382 ms\n",
            "Divergent Time: 1.019 ms\n",
            "-------------------------------------\n",
            "Slowdown Factor: 0.74x\n",
            "CONCLUSION: FAILED. Check compiler settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 2: A100 is mocking me**\n",
        "\n",
        "- A100 has Independent Thread Scheduling.\n",
        "\n",
        "- Old GPUs like Pascal, the warp was locked. If threads divereged, the hardware physically couldn't manage the others\n",
        "\n",
        "- A100 maintains a separate Program Counter for every thread. While it should have serialized it allowed some overlap even inside a single warp.\n",
        "\n",
        "## **Now I'm gonna stop asking nicely with nanosleep to scheduler, and I'm gonna force the threads to calculate math in a while loop using GPU's raw clock cycles (clock64()).**\n",
        "\n",
        "The Math units ALU cannot be shared. If 16 threads are using the ALU the other 16 must wait. There's no magic scheduler that can create extra ALU's out of thin air.\n",
        "\n"
      ],
      "metadata": {
        "id": "PP-NewMj-oFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile warp_divergence_v2.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Helper: Freezes the core for 'ns' nanoseconds\n",
        "__device__ void burn_time(long ns) {\n",
        "    __nanosleep(ns);\n",
        "}\n",
        "\n",
        "// --- WARMUP KERNEL ---\n",
        "// Wakes up the GPU so the first real test doesn't pay the penalty.\n",
        "__global__ void warmup() {\n",
        "    int tid = threadIdx.x;\n",
        "    if (tid == 0) burn_time(10000);\n",
        "}\n",
        "\n",
        "// --- KERNEL A: Coherent ---\n",
        "__global__ void coherent_kernel() {\n",
        "    int tid = threadIdx.x;\n",
        "    if (tid < 32) {\n",
        "        burn_time(1000000); // 1ms\n",
        "    } else {\n",
        "        burn_time(1000000); // Dead code\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- KERNEL B: Divergent ---\n",
        "__global__ void divergent_kernel() {\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (tid % 2 == 0) {\n",
        "        burn_time(1000000); // Evens: 1,000,000 ns\n",
        "    } else {\n",
        "        // TRICK: Change the value by 100ns.\n",
        "        // The compiler sees different arguments and CANNOT merge the branches.\n",
        "        // It is forced to generate two separate paths.\n",
        "        burn_time(1000100); // Odds:  1,000,100 ns\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float time_coherent, time_divergent;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    printf(\"--- WARP DIVERGENCE BENCHMARK V2 ---\\n\");\n",
        "\n",
        "    // 1. WARMUP (Crucial Step)\n",
        "    warmup<<<1, 32>>>();\n",
        "    cudaDeviceSynchronize(); // Wait for warmup to finish\n",
        "\n",
        "    // 2. Measure Coherent\n",
        "    cudaEventRecord(start);\n",
        "    coherent_kernel<<<1, 32>>>();\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_coherent, start, stop);\n",
        "    printf(\"Coherent Time:  %.3f ms\\n\", time_coherent);\n",
        "\n",
        "    // 3. Measure Divergent\n",
        "    cudaEventRecord(start);\n",
        "    divergent_kernel<<<1, 32>>>();\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_divergent, start, stop);\n",
        "    printf(\"Divergent Time: %.3f ms\\n\", time_divergent);\n",
        "\n",
        "    // 4. Analysis\n",
        "    float slowdown = time_divergent / time_coherent;\n",
        "    printf(\"Slowdown Factor: %.2fx\\n\", slowdown);\n",
        "\n",
        "    if (slowdown > 1.9) {\n",
        "        printf(\"CONCLUSION: SUCCESS. Serialized (2 Trips).\\n\");\n",
        "    } else {\n",
        "        printf(\"CONCLUSION: FAILED. Still optimized.\\n\");\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSJkN4Ju-rVL",
        "outputId": "0bc13f33-9526-45a5-b370-0996b81e4594"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing warp_divergence_v2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 warp_divergence_v2.cu -o warp_divergence_v2\n",
        "!./warp_divergence_v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVf9OBGT-y9u",
        "outputId": "001938ac-5283-402a-9ff5-a19eb0273dc6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- WARP DIVERGENCE BENCHMARK V2 ---\n",
            "Coherent Time:  0.762 ms\n",
            "Divergent Time: 1.016 ms\n",
            "Slowdown Factor: 1.33x\n",
            "CONCLUSION: FAILED. Still optimized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 3: A100, Take That!**\n",
        "\n",
        "- clock64() reads the actual hardware tick. By forcing a while loop on it, we are jamming the execution pipeline. The \"Odd\" threads cannot use the pipeline until the \"Even\" threads release it."
      ],
      "metadata": {
        "id": "gapIJAwI8xRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile warp_divergence_v3.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --- THE SPIN LOCK ---\n",
        "// Instead of sleeping, we BURN cycles.\n",
        "// We continuously check the clock. This occupies the ALU 100%.\n",
        "// It prevents the scheduler from doing anything else.\n",
        "__device__ void burn_cycles(long long cycles) {\n",
        "    long long start = clock64();\n",
        "    while (clock64() - start < cycles) {\n",
        "        // Spin!\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- KERNEL A: Coherent ---\n",
        "// Everyone burns 10,000,000 cycles (~7-8ms on A100)\n",
        "__global__ void coherent_kernel() {\n",
        "    int tid = threadIdx.x;\n",
        "    if (tid < 32) {\n",
        "        burn_cycles(10000000);\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- KERNEL B: Divergent ---\n",
        "// Evens burn 10M. Odds burn 10M.\n",
        "// Since they share the SAME ALU execution unit, they MUST run sequentially.\n",
        "__global__ void divergent_kernel() {\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (tid % 2 == 0) {\n",
        "        burn_cycles(10000000);\n",
        "    } else {\n",
        "        // We use a separate function call/block to ensure no merging\n",
        "        burn_cycles(10000000);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float time_coherent, time_divergent;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    printf(\"--- WARP DIVERGENCE: ALU SPIN TEST ---\\n\");\n",
        "    printf(\"Task: Burn 10,000,000 clock cycles.\\n\");\n",
        "\n",
        "    // 1. WARMUP\n",
        "    coherent_kernel<<<1, 32>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // 2. Measure Coherent\n",
        "    cudaEventRecord(start);\n",
        "    coherent_kernel<<<1, 32>>>();\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_coherent, start, stop);\n",
        "    printf(\"Coherent Time:  %.3f ms\\n\", time_coherent);\n",
        "\n",
        "    // 3. Measure Divergent\n",
        "    cudaEventRecord(start);\n",
        "    divergent_kernel<<<1, 32>>>();\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_divergent, start, stop);\n",
        "    printf(\"Divergent Time: %.3f ms\\n\", time_divergent);\n",
        "\n",
        "    // 4. Analysis\n",
        "    float ratio = time_divergent / time_coherent;\n",
        "    printf(\"Slowdown Factor: %.2fx\\n\", ratio);\n",
        "\n",
        "    if (ratio > 1.95) {\n",
        "        printf(\"CONCLUSION: SUCCESS. The ALUs were serialized.\\n\");\n",
        "    } else {\n",
        "        printf(\"CONCLUSION: WTF.\\n\");\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mUErofoApuz",
        "outputId": "d2f3f301-804d-49ea-b226-b11f818057ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing warp_divergence_v3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 warp_divergence_v3.cu -o warp_divergence_v3\n",
        "!./warp_divergence_v3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NrjY_tOAr26",
        "outputId": "fb8bdace-1982-46e6-b07f-d5b960702f03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- WARP DIVERGENCE: ALU SPIN TEST ---\n",
            "Task: Burn 10,000,000 clock cycles.\n",
            "Coherent Time:  9.144 ms\n",
            "Divergent Time: 18.298 ms\n",
            "Slowdown Factor: 2.00x\n",
            "CONCLUSION: SUCCESS. The ALUs were serialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stride**:"
      ],
      "metadata": {
        "id": "D0ZVR2HpCOsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "tid * 1: Every thread takes the very next number. (Walking).\n",
        "\n",
        "tid * 32: Every thread jumps 32 spots away from its neighbor. (Long Jumping).\n",
        "\n",
        "**Why it kills performance:** The Memory Controller hates Long Jumps. It has to calculate a new address for every single thread instead of just saying \"Give me the whole block.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Yvq14qrNCVkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile coalescing_stride.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void stride_kernel(float *data, int stride, int N) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    long long idx = (long long)tid * stride;\n",
        "\n",
        "    if (idx < N) {\n",
        "        data[idx] += 1.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 128 * 1024 * 1024; // 128 Million Floats\n",
        "    size_t bytes = N * sizeof(float);\n",
        "    float *d_data;\n",
        "    cudaMalloc(&d_data, bytes);\n",
        "\n",
        "    int strides[] = {1, 32}; // Comparing Best vs Worst\n",
        "\n",
        "    printf(\"--- STRIDE BENCHMARK ---\\n\");\n",
        "\n",
        "    for (int i = 0; i < 2; i++) {\n",
        "        int s = strides[i];\n",
        "        int elements_touched = N / s;\n",
        "        int numBlocks = (elements_touched + 256 - 1) / 256;\n",
        "\n",
        "        cudaEvent_t start, stop;\n",
        "        cudaEventCreate(&start); cudaEventCreate(&stop);\n",
        "\n",
        "        cudaEventRecord(start);\n",
        "        stride_kernel<<<numBlocks, 256>>>(d_data, s, N);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "        // Calculate Bandwidth\n",
        "        double total_bytes = (double)elements_touched * 4 * 2;\n",
        "        double gb_per_sec = (total_bytes / (milliseconds / 1000.0)) / 1e9;\n",
        "\n",
        "        printf(\"Stride %2d:  %6.2f GB/s\\n\", s, gb_per_sec);\n",
        "    }\n",
        "    cudaFree(d_data);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aKCIe0cCZ_m",
        "outputId": "c4c611a3-0175-4561-e690-b9d6b18a735a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coalescing_stride.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 coalescing_stride.cu -o coalescing_test\n",
        "!./coalescing_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgdrB9qUCapl",
        "outputId": "c6ac9369-2ad5-4751-ecc9-01a17d08157f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STRIDE BENCHMARK ---\n",
            "Stride  1:  1167.68 GB/s\n",
            "Stride 32:   80.12 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1.3: The Single-Thread Juggler.\n",
        "\n",
        "- Usually, GPUs hide latency by swapping threads (Thread Level Parallelism). If Thread 1 is waiting for memory, the GPU runs Thread 2. But what if you only have 1 thread?\n",
        "\n",
        "- You have to use ILP (Instruction Level Parallelism). This is like a juggler.\n",
        "\n",
        "- 1. Serial (Bad): Throw Ball 1. Watch it fly. Catch it. Throw Ball 2.\n",
        "\n",
        "- 2. ILP (Good): Throw Ball 1. Throw Ball 2. Throw Ball 3. Catch Ball 1. Catch Ball 2...\n",
        "\n",
        "- We are going to force 1 Warp to process a huge array.\n",
        "\n",
        "- Kernel A: Loads one value, squares it, stores it. (Stop-and-Go).\n",
        "\n",
        "- Kernel B: Loads 4 values at once, squares them, stores them. (Pipelined)."
      ],
      "metadata": {
        "id": "_XeNkpNWDzd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We expected Kernel B to be 2x to 4x faster.**\n",
        "\n",
        "## **But here's an 8.47x speedup.**\n",
        "\n",
        "- That happened due to Vectorization.\n",
        "The Compiler saw we asked for x, y, z, w (4 floats) right next to each other. Instead of issuing 4 separate \"Load Float\" instructions, it fused them into One Giant \"Load Float4\" Instruction.\n",
        "\n",
        "- Serial Kernel: 4 Instructions. 4 Address Calculations. 4 Latency penalties.\n",
        "\n",
        "- ILP Kernel: 1 Instruction. 1 Address Calculation. 1 Latency penalty.\n",
        "\n",
        "This didn't just hide latency; it slashed the instruction count by 75%."
      ],
      "metadata": {
        "id": "Lru_PdgmNNqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i += stride\n",
        "\n",
        "- stride is the Total Number of Threads in the grid (blockDim.x * gridDim.x)\n",
        "\n",
        "- After a thread finishes its first job, it \"jumps\" forward by the total number of threads to find its next job."
      ],
      "metadata": {
        "id": "legnwxu6OQif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ilp_unrolling.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// --- KERNEL A: Serial (The One-Ball Juggler) ---\n",
        "// Load -> Stall -> Math -> Store -> Stall -> Repeat\n",
        "__global__ void serial_kernel(float *data, int N) {\n",
        "    int tid = threadIdx.x;\n",
        "    int stride = blockDim.x;\n",
        "\n",
        "    for (int i = tid; i < N; i += stride) {\n",
        "        float x = data[i];      // Load (Latency: 400 cycles)\n",
        "        x = x * x;              // Math (Dependent on Load)\n",
        "        data[i] = x;            // Store\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- KERNEL B: ILP Unrolled (The Four-Ball Juggler) ---\n",
        "// Load 4 items. The GPU issues all 4 loads before the first one comes back.\n",
        "// While waiting for 'x', it issues the request for 'y'.\n",
        "__global__ void ilp_kernel(float *data, int N) {\n",
        "    int tid = threadIdx.x;\n",
        "    int stride = blockDim.x;\n",
        "\n",
        "    // Process 4 elements per loop iteration\n",
        "    for (int i = tid; i < N; i += stride * 4) {\n",
        "        // 1. Issue 4 Loads immediately (Pipelining)\n",
        "        float x = data[i];\n",
        "        float y = data[i + stride];\n",
        "        float z = data[i + stride * 2];\n",
        "        float w = data[i + stride * 3];\n",
        "\n",
        "        // 2. Compute (Independent Math)\n",
        "        x = x * x;\n",
        "        y = y * y;\n",
        "        z = z * z;\n",
        "        w = w * w;\n",
        "\n",
        "        // 3. Store\n",
        "        data[i] = x;\n",
        "        data[i + stride] = y;\n",
        "        data[i + stride * 2] = z;\n",
        "        data[i + stride * 3] = w;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1000000; // 1 Million floats\n",
        "    size_t bytes = N * sizeof(float);\n",
        "\n",
        "    float *d_data;\n",
        "    cudaMalloc(&d_data, bytes);\n",
        "\n",
        "    printf(\"--- ILP LATENCY HIDING BENCHMARK ---\\n\");\n",
        "    printf(\"Constraint: Running with ONLY 1 WARP (32 Threads).\\n\");\n",
        "    printf(\"The GPU cannot hide latency by switching warps.\\n\");\n",
        "    printf(\"It MUST use Instruction Level Parallelism.\\n\\n\");\n",
        "\n",
        "    // Events for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start); cudaEventCreate(&stop);\n",
        "    float ms_serial, ms_ilp;\n",
        "\n",
        "    // 1. Run Serial\n",
        "    cudaEventRecord(start);\n",
        "    serial_kernel<<<1, 32>>>(d_data, N); // 1 Block, 32 Threads\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&ms_serial, start, stop);\n",
        "\n",
        "    printf(\"Serial Time: %6.2f ms\\n\", ms_serial);\n",
        "\n",
        "    // 2. Run ILP\n",
        "    cudaEventRecord(start);\n",
        "    ilp_kernel<<<1, 32>>>(d_data, N);    // 1 Block, 32 Threads\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&ms_ilp, start, stop);\n",
        "\n",
        "    printf(\"ILP Time:    %6.2f ms\\n\", ms_ilp);\n",
        "\n",
        "    // 3. Results\n",
        "    printf(\"\\nSpeedup: %.2fx\\n\", ms_serial / ms_ilp);\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRvrwF3VECSn",
        "outputId": "2c7bfda3-de51-4b59-f85a-0cea196d0cb9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ilp_unrolling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_80 ilp_unrolling.cu -o ilp_test\n",
        "!./ilp_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU2N8ru4EDy1",
        "outputId": "0c3fdeef-81ad-45f8-dbdd-11872dccd8c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ILP LATENCY HIDING BENCHMARK ---\n",
            "Constraint: Running with ONLY 1 WARP (32 Threads).\n",
            "The GPU cannot hide latency by switching warps.\n",
            "It MUST use Instruction Level Parallelism.\n",
            "\n",
            "Serial Time:  15.73 ms\n",
            "ILP Time:      1.86 ms\n",
            "\n",
            "Speedup: 8.47x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mFsFPI5SMVDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mD47vTWGMVGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ilp_unrolling.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "__global__ void serial_kernel(float *data, int N){\n",
        "  int tid = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = tid, i < N; i+= stride){\n",
        "    float x = data[i];\n",
        "    x = x * x;\n",
        "    data[i] = x;\n",
        "\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "tHE9x8C0MVNr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}